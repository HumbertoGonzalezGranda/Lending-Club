{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and Hyperparamter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select(query):\n",
    "    \n",
    "    conn = sqlite3.connect('./data/lending-club-loan-data/database2.sqlite')\n",
    "    cursor = conn.cursor()\n",
    "    temp_df = pd.DataFrame(cursor.execute(query).fetchall())\n",
    "    temp_df.columns = list(map(lambda x: x[0], cursor.description))\n",
    "    conn.close()\n",
    "    \n",
    "    return temp_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = select('SELECT * FROM FEATURES_TRAIN')\n",
    "targets_train = select('SELECT * FROM TARGETS_TRAIN')\n",
    "features_test = select('SELECT * FROM FEATURES_TEST')\n",
    "targets_test = select('SELECT * FROM TARGETS_TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our scoring functions will all focus on the negative class -- we already know the model performs well on the positive class\n",
    "# we are looking to optimize model performance on the negative examples. we will eventually have to consider model\n",
    "# performance on the positive class as well -- can't sacrifice too much of the predictability on positives\n",
    "\n",
    "# F1 score on the negative class may intuitively seem to be the best metric to optimize for, but in the context of loan\n",
    "# portfolio optimization, it may be more important to detect bad loans -- i.e. maximize recall on the negative class,\n",
    "# even if it means reduced precision and F1 (classifying many good loans as bad)\n",
    "\n",
    "def neg_f1(targets_true,targets_predicted):\n",
    "    tn, fp, fn, tp = confusion_matrix(targets_true,targets_predicted).ravel()\n",
    "    precision = tn/(tn+fn)\n",
    "    recall = tn/(tn+fp)\n",
    "    return 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "def neg_precision(targets_true,targets_predicted):\n",
    "    tn, fp, fn, tp = confusion_matrix(targets_true,targets_predicted).ravel()\n",
    "    return tn/(tn+fn)\n",
    "\n",
    "def neg_recall(targets_true,targets_predicted):\n",
    "    tn, fp, fn, tp = confusion_matrix(targets_true,targets_predicted).ravel()\n",
    "    return tn/(tn+fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00044777790216052836"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(features_train,targets_train.loan_status)\n",
    "neg_f1(targets_test.loan_status,lr.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize for F1, precision, recall\n",
    "\n",
    "params = {'C':[.001,.01,.1,1,10,100]}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "clfF1 = GridSearchCV(lr,param_grid=params,scoring=make_scorer(neg_f1),return_train_score=True)\\\n",
    "    .fit(features_train,targets_train.loan_status)\n",
    "clfPREC = GridSearchCV(lr,param_grid=params,scoring=make_scorer(neg_precision),return_train_score=True)\\\n",
    "    .fit(features_train,targets_train.loan_status)\n",
    "clfREC = GridSearchCV(lr,param_grid=params,scoring=make_scorer(neg_recall),return_train_score=True)\\\n",
    "    .fit(features_train,targets_train.loan_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013888</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012683</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006705</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002172</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001117</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score param_C\n",
       "3         0.013888       1\n",
       "0         0.012683   0.001\n",
       "2         0.006705     0.1\n",
       "4         0.002172      10\n",
       "5         0.001117     100\n",
       "1         0.000838    0.01"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we see in the three cells below that regularization on logistic regression does not do much to help the performance\n",
    "# of the model. the effect of the inherent data imbalance greatly limits performance\n",
    "\n",
    "pd.DataFrame(clfF1.cv_results_).sort_values('mean_test_score',ascending=False)[['mean_test_score','param_C']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.548325</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.539683</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.498932</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.480212</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.451000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.437638</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score param_C\n",
       "0         0.548325   0.001\n",
       "5         0.539683     100\n",
       "1         0.498932    0.01\n",
       "2         0.480212     0.1\n",
       "4         0.451000      10\n",
       "3         0.437638       1"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the two best C values are the opposite edge cases.. odd behavior\n",
    "\n",
    "pd.DataFrame(clfPREC.cv_results_).sort_values('mean_test_score',ascending=False)[['mean_test_score','param_C']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006596</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001090</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000559</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score param_C\n",
       "3         0.007239       1\n",
       "0         0.006596   0.001\n",
       "2         0.003410     0.1\n",
       "4         0.001090      10\n",
       "5         0.000559     100\n",
       "1         0.000419    0.01"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clfREC.cv_results_).sort_values('mean_test_score',ascending=False)[['mean_test_score','param_C']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000447777902161\n",
      "0.466666666667\n",
      "0.000223989248516\n"
     ]
    }
   ],
   "source": [
    "# models perform with respect to F1, precision, and recall about as poorly as they did without regularization\n",
    "\n",
    "print(neg_f1(targets_test.loan_status,clfF1.best_estimator_.predict(features_test)))\n",
    "print(neg_precision(targets_test.loan_status,clfPREC.best_estimator_.predict(features_test)))\n",
    "print(neg_recall(targets_test.loan_status,clfREC.best_estimator_.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LRF1 = neg_f1(targets_test.loan_status,clfF1.best_estimator_.predict(features_test))\n",
    "LRPREC = neg_precision(targets_test.loan_status,clfPREC.best_estimator_.predict(features_test))\n",
    "LRREC = neg_recall(targets_test.loan_status,clfREC.best_estimator_.predict(features_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Gaussian Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# when estimated by the data (by default), class prior probabilities of GNB are the proportions of each class\n",
    "# we can tinker with the prior probabilities to increase the model's sensitivity to certain classes. this should\n",
    "# intuitively have a similar effect as resampling the dataset input to GNB without adjusting the default priors\n",
    "\n",
    "# the first value in each pair is the prior probability of the negative class\n",
    "\n",
    "params = {'priors':[[0.1,0.9],[0.2,0.8],[0.3,0.7],[0.4,0.6],[0.5,0.5],[0.6,0.4]]}\n",
    "\n",
    "GNB = GaussianNB()\n",
    "\n",
    "clfF1 = GridSearchCV(GNB,param_grid=params,scoring=make_scorer(neg_f1),return_train_score=True)\\\n",
    "    .fit(features_train,targets_train.loan_status)\n",
    "clfPREC = GridSearchCV(GNB,param_grid=params,scoring=make_scorer(neg_precision),return_train_score=True)\\\n",
    "    .fit(features_train,targets_train.loan_status)\n",
    "clfREC = GridSearchCV(GNB,param_grid=params,scoring=make_scorer(neg_recall),return_train_score=True)\\\n",
    "    .fit(features_train,targets_train.loan_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_priors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.373224</td>\n",
       "      <td>[0.5, 0.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.367746</td>\n",
       "      <td>[0.6, 0.4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.367621</td>\n",
       "      <td>[0.4, 0.6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.351018</td>\n",
       "      <td>[0.3, 0.7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.327848</td>\n",
       "      <td>[0.2, 0.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.268723</td>\n",
       "      <td>[0.1, 0.9]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score param_priors\n",
       "4         0.373224   [0.5, 0.5]\n",
       "5         0.367746   [0.6, 0.4]\n",
       "3         0.367621   [0.4, 0.6]\n",
       "2         0.351018   [0.3, 0.7]\n",
       "1         0.327848   [0.2, 0.8]\n",
       "0         0.268723   [0.1, 0.9]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for a balance between precision and recall, it seems about an even balance on prior probabilities is optimal\n",
    "\n",
    "pd.DataFrame(clfF1.cv_results_).sort_values('mean_test_score',ascending=False)[['mean_test_score','param_priors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_priors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.357363</td>\n",
       "      <td>[0.1, 0.9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.325372</td>\n",
       "      <td>[0.2, 0.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.302458</td>\n",
       "      <td>[0.3, 0.7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.285865</td>\n",
       "      <td>[0.4, 0.6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.267287</td>\n",
       "      <td>[0.5, 0.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.246101</td>\n",
       "      <td>[0.6, 0.4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score param_priors\n",
       "0         0.357363   [0.1, 0.9]\n",
       "1         0.325372   [0.2, 0.8]\n",
       "2         0.302458   [0.3, 0.7]\n",
       "3         0.285865   [0.4, 0.6]\n",
       "4         0.267287   [0.5, 0.5]\n",
       "5         0.246101   [0.6, 0.4]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# precision and recall have opposite orderings of param values. it seems that we can reduce precision for greater recall\n",
    "# and vice versa. indeed, this is a very similar result to what happened with undersampling\n",
    "\n",
    "pd.DataFrame(clfPREC.cv_results_).sort_values('mean_test_score',ascending=False)[['mean_test_score','param_priors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_priors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.728144</td>\n",
       "      <td>[0.6, 0.4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.619088</td>\n",
       "      <td>[0.5, 0.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.515315</td>\n",
       "      <td>[0.4, 0.6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.418306</td>\n",
       "      <td>[0.3, 0.7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.330436</td>\n",
       "      <td>[0.2, 0.8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.215344</td>\n",
       "      <td>[0.1, 0.9]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score param_priors\n",
       "5         0.728144   [0.6, 0.4]\n",
       "4         0.619088   [0.5, 0.5]\n",
       "3         0.515315   [0.4, 0.6]\n",
       "2         0.418306   [0.3, 0.7]\n",
       "1         0.330436   [0.2, 0.8]\n",
       "0         0.215344   [0.1, 0.9]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clfREC.cv_results_).sort_values('mean_test_score',ascending=False)[['mean_test_score','param_priors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.367787323277\n",
      "0.35449833395\n",
      "0.722589315713\n"
     ]
    }
   ],
   "source": [
    "# f1 and recall are much better than in logistic regression, as seen in the baseline model w/o resampling\n",
    "\n",
    "print(neg_f1(targets_test.loan_status,clfF1.best_estimator_.predict(features_test)))\n",
    "print(neg_precision(targets_test.loan_status,clfPREC.best_estimator_.predict(features_test)))\n",
    "print(neg_recall(targets_test.loan_status,clfREC.best_estimator_.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GNBF1 = neg_f1(targets_test.loan_status,clfF1.best_estimator_.predict(features_test))\n",
    "GNBPREC = neg_precision(targets_test.loan_status,clfPREC.best_estimator_.predict(features_test))\n",
    "GNBREC = neg_recall(targets_test.loan_status,clfREC.best_estimator_.predict(features_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### k Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this takes a while to compute.. commenting to remove code from future runs\n",
    "\n",
    "params = {'n_neighbors':list(range(1,11))}\n",
    "\n",
    "kNN = KNeighborsClassifier()\n",
    "\n",
    "# clfF1 = GridSearchCV(kNN,param_grid=params,scoring=make_scorer(neg_f1),return_train_score=True)\\\n",
    "#    .fit(features_train,targets_train.loan_status)\n",
    "#clfPREC = GridSearchCV(kNN,param_grid=params,scoring=make_scorer(neg_precision),return_train_score=True)\\\n",
    "#    .fit(features_train,targets_train.loan_status)\n",
    "#clfREC = GridSearchCV(kNN,param_grid=params,scoring=make_scorer(neg_recall),return_train_score=True)\\\n",
    "#    .fit(features_train,targets_train.loan_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_n_neighbors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.255185</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.205052</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.203513</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.152544</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.150728</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.119035</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.107831</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.091316</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.081866</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.062036</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score param_n_neighbors\n",
       "1         0.255185                 2\n",
       "3         0.205052                 4\n",
       "0         0.203513                 1\n",
       "5         0.152544                 6\n",
       "2         0.150728                 3\n",
       "7         0.119035                 8\n",
       "4         0.107831                 5\n",
       "9         0.091316                10\n",
       "6         0.081866                 7\n",
       "8         0.062036                 9"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overall performance is much weaker than Gaussian Naive-Bayes\n",
    "\n",
    "pd.DataFrame(clfF1.cv_results_).sort_values('mean_test_score',ascending=False)[['mean_test_score','param_n_neighbors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_n_neighbors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.259177</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.257330</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.248811</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.246623</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.230260</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.228096</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.221454</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.218699</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.203898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.198596</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score param_n_neighbors\n",
       "9         0.259177                10\n",
       "8         0.257330                 9\n",
       "6         0.248811                 7\n",
       "7         0.246623                 8\n",
       "4         0.230260                 5\n",
       "5         0.228096                 6\n",
       "2         0.221454                 3\n",
       "3         0.218699                 4\n",
       "0         0.203898                 1\n",
       "1         0.198596                 2"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# greater number of neighbors tends to increase precision of model\n",
    "\n",
    "pd.DataFrame(clfPREC.cv_results_).sort_values('mean_test_score',ascending=False)[['mean_test_score','param_n_neighbors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33333333333333331"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# increases in precision seem to diminish quickly on both the test set and training set\n",
    "\n",
    "kNN1 = KNeighborsClassifier(n_neighbors=70).fit(features_train,targets_train.loan_status)\n",
    "neg_precision(targets_test.loan_status,kNN1.predict(features_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_n_neighbors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.356875</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.203130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.193013</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.114589</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.114254</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.078452</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.070402</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.055422</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.048994</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.035271</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score param_n_neighbors\n",
       "1         0.356875                 2\n",
       "0         0.203130                 1\n",
       "3         0.193013                 4\n",
       "5         0.114589                 6\n",
       "2         0.114254                 3\n",
       "7         0.078452                 8\n",
       "4         0.070402                 5\n",
       "9         0.055422                10\n",
       "6         0.048994                 7\n",
       "8         0.035271                 9"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clfREC.cv_results_).sort_values('mean_test_score',ascending=False)[['mean_test_score','param_n_neighbors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25316759263\n",
      "0.263157894737\n",
      "0.354686975025\n"
     ]
    }
   ],
   "source": [
    "print(neg_f1(targets_test.loan_status,clfF1.best_estimator_.predict(features_test)))\n",
    "print(neg_precision(targets_test.loan_status,clfPREC.best_estimator_.predict(features_test)))\n",
    "print(neg_recall(targets_test.loan_status,clfREC.best_estimator_.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kNNF1 = neg_f1(targets_test.loan_status,clfF1.best_estimator_.predict(features_test))\n",
    "kNNPREC = neg_precision(targets_test.loan_status,clfPREC.best_estimator_.predict(features_test))\n",
    "kNNREC = neg_recall(targets_test.loan_status,clfREC.best_estimator_.predict(features_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more estimators (trees) in a RF generally result in better performance, at computational expense. we leave this out\n",
    "# of the grid search (too long to compute, otherwise) and isolate the other parameters\n",
    "# param values are chosen somewhat haphazardly for this initial run. simply want to see trends w/ varying values\n",
    "# choosing few values because this is too computationally expensive to run on more\n",
    "\n",
    "# also commenting to remove from future runs\n",
    "\n",
    "params = {'min_samples_split':[2,8,32],'min_samples_leaf':[1,16,32]}\n",
    "\n",
    "RF = RandomForestClassifier()\n",
    "\n",
    "#clfF1 = GridSearchCV(RF,param_grid=params,scoring=make_scorer(neg_f1),return_train_score=True)\\\n",
    "#    .fit(features_train,targets_train.loan_status)\n",
    "#clfPREC = GridSearchCV(RF,param_grid=params,scoring=make_scorer(neg_precision),return_train_score=True)\\\n",
    "#    .fit(features_train,targets_train.loan_status)\n",
    "#clfREC = GridSearchCV(RF,param_grid=params,scoring=make_scorer(neg_recall),return_train_score=True)\\\n",
    "#    .fit(features_train,targets_train.loan_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.214364</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.151885</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100325</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.073562</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.070909</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070814</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.050522</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.049800</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.045941</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score param_min_samples_leaf param_min_samples_split\n",
       "0         0.214364                      1                       2\n",
       "1         0.151885                      1                       8\n",
       "2         0.100325                      1                      32\n",
       "4         0.073562                     16                       8\n",
       "5         0.070909                     16                      32\n",
       "3         0.070814                     16                       2\n",
       "7         0.050522                     32                       8\n",
       "6         0.049800                     32                       2\n",
       "8         0.045941                     32                      32"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for both F1 and recall, it seems tweaking the min samples does not help model performance \n",
    "# (in fact it seems to greatly reduce it..) the default values of 1 and 2 seem best\n",
    "\n",
    "pd.DataFrame(clfF1.cv_results_).sort_values('mean_test_score',ascending=False)[['mean_test_score',\n",
    "                                                                                'param_min_samples_leaf',\n",
    "                                                                                'param_min_samples_split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.488115</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.478883</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.475752</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.468616</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.467752</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.464399</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.429080</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.374922</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.336879</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score param_min_samples_leaf param_min_samples_split\n",
       "8         0.488115                     32                      32\n",
       "6         0.478883                     32                       2\n",
       "7         0.475752                     32                       8\n",
       "3         0.468616                     16                       2\n",
       "5         0.467752                     16                      32\n",
       "4         0.464399                     16                       8\n",
       "2         0.429080                      1                      32\n",
       "1         0.374922                      1                       8\n",
       "0         0.336879                      1                       2"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for precision, however, adjusting the min samples in a leaf seems to greatly increase the model's performance\n",
    "\n",
    "pd.DataFrame(clfPREC.cv_results_).sort_values('mean_test_score',ascending=False)[['mean_test_score',\n",
    "                                                                                'param_min_samples_leaf',\n",
    "                                                                                'param_min_samples_split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "356\n"
     ]
    }
   ],
   "source": [
    "# the precision performance jumps around quite a bit as we vary minimum samples in leaeves, \n",
    "# but we are able to achieve an 'OK' level of precision (around ~0.5) on the test set..not bad\n",
    "\n",
    "RF1 = RandomForestClassifier(min_samples_leaf=60,random_state=10).fit(features_train,targets_train.loan_status)\n",
    "print(neg_precision(targets_test.loan_status,RF1.predict(features_test)))\n",
    "print((RF1.predict(features_test)-1).sum()*-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.158077</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.092286</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.056931</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.039352</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038206</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.037479</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.026495</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.026355</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.025601</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score param_min_samples_leaf param_min_samples_split\n",
       "0         0.158077                      1                       2\n",
       "1         0.092286                      1                       8\n",
       "2         0.056931                      1                      32\n",
       "5         0.039352                     16                      32\n",
       "3         0.038206                     16                       2\n",
       "4         0.037479                     16                       8\n",
       "6         0.026495                     32                       2\n",
       "8         0.026355                     32                      32\n",
       "7         0.025601                     32                       8"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clfREC.cv_results_).sort_values('mean_test_score',ascending=False)[['mean_test_score',\n",
    "                                                                                'param_min_samples_leaf',\n",
    "                                                                                'param_min_samples_split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.207618314736\n",
      "0.479289940828\n",
      "0.157464441707\n"
     ]
    }
   ],
   "source": [
    "print(neg_f1(targets_test.loan_status,clfF1.best_estimator_.predict(features_test)))\n",
    "print(neg_precision(targets_test.loan_status,clfPREC.best_estimator_.predict(features_test)))\n",
    "print(neg_recall(targets_test.loan_status,clfREC.best_estimator_.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RFF1 = neg_f1(targets_test.loan_status,clfF1.best_estimator_.predict(features_test))\n",
    "RFPREC = neg_precision(targets_test.loan_status,clfPREC.best_estimator_.predict(features_test))\n",
    "RFREC = neg_recall(targets_test.loan_status,clfREC.best_estimator_.predict(features_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
