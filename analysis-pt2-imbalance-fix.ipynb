{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of test set predictive results after basic resampling techniques on training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select(query):\n",
    "    \n",
    "    conn = sqlite3.connect('./data/lending-club-loan-data/database2.sqlite')\n",
    "    cursor = conn.cursor()\n",
    "    temp_df = pd.DataFrame(cursor.execute(query).fetchall())\n",
    "    temp_df.columns = list(map(lambda x: x[0], cursor.description))\n",
    "    conn.close()\n",
    "    \n",
    "    return temp_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = select('SELECT * FROM FEATURES_TRAIN')\n",
    "targets_train = select('SELECT * FROM TARGETS_TRAIN')\n",
    "features_test = select('SELECT * FROM FEATURES_TEST')\n",
    "targets_test = select('SELECT * FROM TARGETS_TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2153942473241268"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training data has approx 1:5 ratio of minority to majority class examples\n",
    "\n",
    "ratio = -1*((targets_train.loan_status-1).sum())/targets_train.loan_status.sum()\n",
    "ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set Manipulation approach 1: Undersampling of the majority class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ratio_dict1: undersample the majority class to create a 1:1 ratio of minority to majority class examples\n",
    "# ratio_dict2: undersample the majority class to create a 2:1 ratio of minority to majority class examples\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "ratio_dict1 = {0:len(features_train[~targets_train.astype(bool).loan_status]),\\\n",
    "               1:len(features_train[~targets_train.astype(bool).loan_status])}\n",
    "ratio_dict2 = {0:len(features_train[~targets_train.astype(bool).loan_status]),\\\n",
    "                1:round((.5*len(features_train[~targets_train.astype(bool).loan_status])))}\n",
    "\n",
    "features_res1, targets_res1 = RandomUnderSampler(ratio=ratio_dict1,random_state=2)\\\n",
    "    .fit_sample(features_train,targets_train.loan_status)\n",
    "    \n",
    "features_res2, targets_res2 = RandomUnderSampler(ratio=ratio_dict2,random_state=2)\\\n",
    "    .fit_sample(features_train,targets_train.loan_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1:1 minority to majority class undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.641775963863\n"
     ]
    }
   ],
   "source": [
    "# accuracy considerably drops -- this same baseline model in pt1 had 82% accuracy on the test set\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(features_res1,targets_res1)\n",
    "print(accuracy_score(lr.predict(features_test),targets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LR\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.64      0.64     35780\n",
      "          1       0.64      0.65      0.65     35780\n",
      "\n",
      "avg / total       0.64      0.64      0.64     71560\n",
      "\n",
      "TEST LR\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.63      0.38      8929\n",
      "          1       0.89      0.64      0.75     41545\n",
      "\n",
      "avg / total       0.78      0.64      0.68     50474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# precision drops from 50% to 28%, but this model in pt1 only made 4 negative class predictions. there remains room for\n",
    "# improvement in precision, but model has become much more sensitive to outputting the minority class\n",
    "# recall is GREATLY improved -- this model in pt1 has close to 0% recall on the test set\n",
    "\n",
    "print('TRAIN LR')\n",
    "print(classification_report(targets_res1,lr.predict(features_res1)))\n",
    "print('TEST LR')\n",
    "print(classification_report(targets_test,lr.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.57      0.37      8929\n",
      "          1       0.88      0.68      0.77     41545\n",
      "\n",
      "avg / total       0.77      0.66      0.70     50474\n",
      "\n",
      "kNN\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.20      0.54      0.29      8929\n",
      "          1       0.85      0.54      0.66     41545\n",
      "\n",
      "avg / total       0.73      0.54      0.60     50474\n",
      "\n",
      "RF\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.26      0.65      0.37      8929\n",
      "          1       0.89      0.61      0.72     41545\n",
      "\n",
      "avg / total       0.78      0.61      0.66     50474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# increase the number of trees in RF from default 10 to 64\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "GNB = GaussianNB().fit(features_res1,targets_res1)\n",
    "KNN = KNeighborsClassifier().fit(features_res1,targets_res1)\n",
    "RF = RandomForestClassifier(n_estimators=64).fit(features_res1,targets_res1)\n",
    "\n",
    "# metrics are for the test set only, for: Gaussian NB, kNN, and Random Forest classifiers\n",
    "\n",
    "# unlike the case without undersampling, these three models offer little performance improvement in any of the metrics\n",
    "# over logistic regression. undersampling seems to greatly improve logistic regression's performance, bringing it up to par\n",
    "# with other algorithms (a bit better, even). still, the other algorithms have notable performance gains from undersampling\n",
    "\n",
    "print('GNB')\n",
    "print(classification_report(targets_test,GNB.predict(features_test)))\n",
    "print('kNN')\n",
    "print(classification_report(targets_test,KNN.predict(features_test)))\n",
    "print('RF')\n",
    "print(classification_report(targets_test,RF.predict(features_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2:1 minority to majority class undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.278341324246\n"
     ]
    }
   ],
   "source": [
    "# accuracy is concerningly low..perhaps we are sacrificing too much in the recall of positive examples\n",
    "\n",
    "lr2 = LogisticRegression()\n",
    "lr2.fit(features_res2,targets_res2)\n",
    "print(accuracy_score(lr2.predict(features_test),targets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.96      0.80     35780\n",
      "          1       0.62      0.13      0.22     17890\n",
      "\n",
      "avg / total       0.67      0.68      0.61     53670\n",
      "\n",
      "TEST             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.19      0.96      0.32      8929\n",
      "          1       0.94      0.13      0.23     41545\n",
      "\n",
      "avg / total       0.81      0.28      0.25     50474\n",
      "\n",
      "test set negative class predictions: 44622\n",
      "test set positive class predictions: 5852\n"
     ]
    }
   ],
   "source": [
    "# recall on the negative class is extremely good, but we have sacrificed too much predictability on the majority class\n",
    "# (the negative class precision is quite low on the test set..the model is now insensitive to positive predictions)\n",
    "\n",
    "# there seems to be a balance that we can achieve between metric performance on positive/negative examples\n",
    "# by changing the ratio of minority to majority class examples, but ideally we want to perform well on both\n",
    "\n",
    "# rather than simply adjusting the ratio of class examples, we move onto other resampling techniques and applying\n",
    "# different algorithms w/ hyperparameter optimization to aim for better performance on both metrics on both classes\n",
    "\n",
    "print('TRAIN'+classification_report(targets_res2,lr2.predict(features_res2)))\n",
    "print('TEST'+classification_report(targets_test,lr2.predict(features_test)))\n",
    "\n",
    "print('test set negative class predictions: '+str((lr2.predict(features_test)-1).sum()*-1))\n",
    "print('test set positive class predictions: '+str((lr2.predict(features_test)).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.24      0.75      0.37      8929\n",
      "          1       0.90      0.50      0.64     41545\n",
      "\n",
      "avg / total       0.79      0.54      0.59     50474\n",
      "\n",
      "kNN\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.19      0.79      0.30      8929\n",
      "          1       0.86      0.27      0.41     41545\n",
      "\n",
      "avg / total       0.74      0.36      0.39     50474\n",
      "\n",
      "RF\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.22      0.88      0.35      8929\n",
      "          1       0.93      0.31      0.46     41545\n",
      "\n",
      "avg / total       0.80      0.41      0.44     50474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note - use this formatting for cleaner output\n",
    "\n",
    "GNB2 = GaussianNB().fit(features_res2,targets_res2)\n",
    "KNN2 = KNeighborsClassifier().fit(features_res2,targets_res2)\n",
    "RF2 = RandomForestClassifier(n_estimators=64).fit(features_res2,targets_res2)\n",
    "\n",
    "print('GNB')\n",
    "print(classification_report(targets_test,GNB2.predict(features_test)))\n",
    "print('kNN')\n",
    "print(classification_report(targets_test,KNN2.predict(features_test)))\n",
    "print('RF')\n",
    "print(classification_report(targets_test,RF2.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regardless of undersampling ratio, precision is fairly poor across the board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we do not explore simple oversampling of the minority class -- known tendency to cause models to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set Manipulation Approach 2: SMOTE (Synthetic Minority Oversampling Technique)\n",
    "generates artifical data points of the minority class at random points in between existing minority class examples (in the features space, using kNN), thus \"filling\" the potential decision region of the minority class (increasing the count) without causing an overfit on already-existing minority class examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\imblearn\\utils\\validation.py:224: UserWarning: After over-sampling, the number of samples (249171) in class 0 will be larger than the number of samples in the majority class (class #1 -> 166114)\n",
      "  n_samples_majority))\n"
     ]
    }
   ],
   "source": [
    "# Ratio1 - 5:8 ratio (approx.), with minority class 3x its original size\n",
    "# Ratio2 - 1:1 ratio, with minority class equal to majority class original size\n",
    "# Ratio3 - 3:2 ratio, with minority class 1.5x time majority class original size\n",
    "\n",
    "SMOTERatio1 = {0:round(3*len(features_train[~targets_train.astype(bool).loan_status])),\\\n",
    "               1:len(features_train[targets_train.astype(bool).loan_status])}\n",
    "\n",
    "SMOTERatio2 = {0:len(features_train[targets_train.astype(bool).loan_status]),\\\n",
    "                1:len(features_train[targets_train.astype(bool).loan_status])}\n",
    "\n",
    "SMOTERatio3 = {0:round(1.5*len(features_train[targets_train.astype(bool).loan_status])),\\\n",
    "                1:len(features_train[targets_train.astype(bool).loan_status])}\n",
    "\n",
    "features_SMOTE1, targets_SMOTE1 = SMOTE(ratio=SMOTERatio1,random_state=2)\\\n",
    "    .fit_sample(features_train,targets_train.loan_status)\n",
    "    \n",
    "features_SMOTE2, targets_SMOTE2 = SMOTE(ratio=SMOTERatio2,random_state=2)\\\n",
    "    .fit_sample(features_train,targets_train.loan_status)\n",
    "    \n",
    "features_SMOTE3, targets_SMOTE3 = SMOTE(ratio=SMOTERatio3,random_state=2)\\\n",
    "    .fit_sample(features_train,targets_train.loan_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrSMOTE1 = LogisticRegression().fit(features_SMOTE1,targets_SMOTE1)\n",
    "lrSMOTE2 = LogisticRegression().fit(features_SMOTE2,targets_SMOTE2)\n",
    "lrSMOTE3 = LogisticRegression().fit(features_SMOTE3,targets_SMOTE3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.740836866506\n",
      "0.638031461743\n",
      "0.470559099735\n"
     ]
    }
   ],
   "source": [
    "# accuracy decreases as we increase the presence of minority classes. again, this is probably because of the \n",
    "# large number of positive classes in the test set\n",
    "\n",
    "print(accuracy_score(lrSMOTE1.predict(features_test),targets_test))\n",
    "print(accuracy_score(lrSMOTE2.predict(features_test),targets_test))\n",
    "print(accuracy_score(lrSMOTE3.predict(features_test),targets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5:8 LR TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.32      0.41      0.36      8929\n",
      "          1       0.86      0.81      0.84     41545\n",
      "\n",
      "avg / total       0.77      0.74      0.75     50474\n",
      "\n",
      "1:1 LR TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.27      0.64      0.38      8929\n",
      "          1       0.89      0.64      0.74     41545\n",
      "\n",
      "avg / total       0.78      0.64      0.68     50474\n",
      "\n",
      "3:2 LR TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.23      0.83      0.36      8929\n",
      "          1       0.92      0.39      0.55     41545\n",
      "\n",
      "avg / total       0.79      0.47      0.52     50474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# altering the proportion of negative/positive classes in the training set through SMOTE seems to have a very similar\n",
    "# effect as doing so through undersampling. we sacrifice recall of the positive class for recall of the negative class,\n",
    "# and we still have poor precision in the negative class. \n",
    "\n",
    "print('5:8 LR TEST')\n",
    "print(classification_report(targets_test,lrSMOTE1.predict(features_test)))\n",
    "print('1:1 LR TEST')\n",
    "print(classification_report(targets_test,lrSMOTE2.predict(features_test)))\n",
    "print('3:2 LR TEST')\n",
    "print(classification_report(targets_test,lrSMOTE3.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GNB_SMOTE1 = GaussianNB().fit(features_SMOTE1,targets_SMOTE1)\n",
    "KNN_SMOTE1 = KNeighborsClassifier().fit(features_SMOTE1,targets_SMOTE1)\n",
    "RF_SMOTE1 = RandomForestClassifier(n_estimators=64).fit(features_SMOTE1,targets_SMOTE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5:8 GNB TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.55      0.37      8929\n",
      "          1       0.88      0.69      0.77     41545\n",
      "\n",
      "avg / total       0.77      0.67      0.70     50474\n",
      "\n",
      "5:8 kNN TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.21      0.33      0.25      8929\n",
      "          1       0.83      0.73      0.78     41545\n",
      "\n",
      "avg / total       0.72      0.66      0.68     50474\n",
      "\n",
      "5:8 RF TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.40      0.10      0.16      8929\n",
      "          1       0.83      0.97      0.90     41545\n",
      "\n",
      "avg / total       0.76      0.81      0.77     50474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# as in the case with undersampling, these other algorithms seem to have no great performance jump compared to logistic\n",
    "# regression. these algorithms seem less sensitive to the proportion of class sizes\n",
    "\n",
    "print('5:8 GNB TEST')\n",
    "print(classification_report(targets_test,GNB_SMOTE1.predict(features_test)))\n",
    "print('5:8 kNN TEST')\n",
    "print(classification_report(targets_test,KNN_SMOTE1.predict(features_test)))\n",
    "print('5:8 RF TEST')\n",
    "print(classification_report(targets_test,RF_SMOTE1.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GNB_SMOTE2 = GaussianNB().fit(features_SMOTE2,targets_SMOTE2)\n",
    "KNN_SMOTE2 = KNeighborsClassifier().fit(features_SMOTE2,targets_SMOTE2)\n",
    "RF_SMOTE2 = RandomForestClassifier(n_estimators=64).fit(features_SMOTE2,targets_SMOTE2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:1 GNB TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.67      0.37      8929\n",
      "          1       0.89      0.57      0.70     41545\n",
      "\n",
      "avg / total       0.78      0.59      0.64     50474\n",
      "\n",
      "1:1 kNN TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.20      0.42      0.27      8929\n",
      "          1       0.84      0.64      0.72     41545\n",
      "\n",
      "avg / total       0.72      0.60      0.64     50474\n",
      "\n",
      "1:1 RF TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.38      0.12      0.18      8929\n",
      "          1       0.83      0.96      0.89     41545\n",
      "\n",
      "avg / total       0.76      0.81      0.77     50474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('1:1 GNB TEST')\n",
    "print(classification_report(targets_test,GNB_SMOTE2.predict(features_test)))\n",
    "print('1:1 kNN TEST')\n",
    "print(classification_report(targets_test,KNN_SMOTE2.predict(features_test)))\n",
    "print('1:1 RF TEST')\n",
    "print(classification_report(targets_test,RF_SMOTE2.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# there seem to be very clear limits/utility in resampling to improve model performance. we explore one more resampling\n",
    "# technique before moving onto other approaches to improving predictive quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set Manipulation Approach 3: ADASYN (Adaptive Synthetic Sampling)\n",
    "similar to SMOTE, but generates artifical minority class examples with a weight for generating more minority class data points in the feature space which are \"difficult\" to learn (noisy, greatly overlapping with majority class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\imblearn\\utils\\validation.py:224: UserWarning: After over-sampling, the number of samples (249171) in class 0 will be larger than the number of samples in the majority class (class #1 -> 166114)\n",
      "  n_samples_majority))\n"
     ]
    }
   ],
   "source": [
    "# Ratio1 - 5:8 ratio (approx.), with minority class 3x its original size\n",
    "# Ratio2 - 1:1 ratio, with minority class equal to majority class original size\n",
    "# Ratio3 - 3:2 ratio, with minority class 1.5x time majority class original size\n",
    "\n",
    "# same ratios as in SMOTE..don't really need to re-instantiate\n",
    "\n",
    "ADASYNRatio1 = {0:round(3*len(features_train[~targets_train.astype(bool).loan_status])),\\\n",
    "               1:len(features_train[targets_train.astype(bool).loan_status])}\n",
    "\n",
    "ADASYNRatio2 = {0:len(features_train[targets_train.astype(bool).loan_status]),\\\n",
    "                1:len(features_train[targets_train.astype(bool).loan_status])}\n",
    "\n",
    "ADASYNRatio3 = {0:round(1.5*len(features_train[targets_train.astype(bool).loan_status])),\\\n",
    "                1:len(features_train[targets_train.astype(bool).loan_status])}\n",
    "\n",
    "features_ADASYN1, targets_ADASYN1 = ADASYN(ratio=ADASYNRatio1,random_state=2)\\\n",
    "    .fit_sample(features_train,targets_train.loan_status)\n",
    "    \n",
    "features_ADASYN2, targets_ADASYN2 = ADASYN(ratio=ADASYNRatio2,random_state=2)\\\n",
    "    .fit_sample(features_train,targets_train.loan_status)\n",
    "    \n",
    "features_ADASYN3, targets_ADASYN3 = ADASYN(ratio=ADASYNRatio3,random_state=2)\\\n",
    "    .fit_sample(features_train,targets_train.loan_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrADASYN1 = LogisticRegression().fit(features_ADASYN1,targets_ADASYN1)\n",
    "lrADASYN2 = LogisticRegression().fit(features_ADASYN2,targets_ADASYN2)\n",
    "lrADASYN3 = LogisticRegression().fit(features_ADASYN3,targets_ADASYN3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.733585608432\n",
      "0.582359234457\n",
      "0.345088560447\n"
     ]
    }
   ],
   "source": [
    "# overall accuracy greatly deterioratres when increasing minority class proportion w ADASYN\n",
    "\n",
    "print(accuracy_score(lrADASYN1.predict(features_test),targets_test))\n",
    "print(accuracy_score(lrADASYN2.predict(features_test),targets_test))\n",
    "print(accuracy_score(lrADASYN3.predict(features_test),targets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5:8 LR TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.31      0.42      0.36      8929\n",
      "          1       0.87      0.80      0.83     41545\n",
      "\n",
      "avg / total       0.77      0.73      0.75     50474\n",
      "\n",
      "1:1 LR TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.26      0.71      0.38      8929\n",
      "          1       0.90      0.55      0.69     41545\n",
      "\n",
      "avg / total       0.79      0.58      0.63     50474\n",
      "\n",
      "3:2 LR TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.20      0.93      0.33      8929\n",
      "          1       0.93      0.22      0.36     41545\n",
      "\n",
      "avg / total       0.80      0.35      0.35     50474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compared to undersampling and SMOTE, ADASYN applied to the training set seems to improve negative class recall the most \n",
    "# effectively. precision is still poor, and about a 1:1 ratio still seems optimal for the tradeoff between positive\n",
    "# class and negative class recall. \n",
    "\n",
    "print('5:8 LR TEST')\n",
    "print(classification_report(targets_test,lrADASYN1.predict(features_test)))\n",
    "print('1:1 LR TEST')\n",
    "print(classification_report(targets_test,lrADASYN2.predict(features_test)))\n",
    "print('3:2 LR TEST')\n",
    "print(classification_report(targets_test,lrADASYN3.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GNB_ADASYN1 = GaussianNB().fit(features_ADASYN1,targets_ADASYN1)\n",
    "KNN_ADASYN1 = KNeighborsClassifier().fit(features_ADASYN1,targets_ADASYN1)\n",
    "RF_ADASYN1 = RandomForestClassifier(n_estimators=64).fit(features_ADASYN1,targets_ADASYN1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5:8 GNB TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.24      0.64      0.35      8929\n",
      "          1       0.88      0.57      0.69     41545\n",
      "\n",
      "avg / total       0.77      0.58      0.63     50474\n",
      "\n",
      "5:8 kNN TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.19      0.28      0.23      8929\n",
      "          1       0.83      0.75      0.79     41545\n",
      "\n",
      "avg / total       0.72      0.66      0.69     50474\n",
      "\n",
      "5:8 RF TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.40      0.08      0.14      8929\n",
      "          1       0.83      0.97      0.90     41545\n",
      "\n",
      "avg / total       0.75      0.82      0.76     50474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# yet again, these algorithms seem to offer very little over Logistic Regression after resampling is performed\n",
    "\n",
    "print('5:8 GNB TEST')\n",
    "print(classification_report(targets_test,GNB_ADASYN1.predict(features_test)))\n",
    "print('5:8 kNN TEST')\n",
    "print(classification_report(targets_test,KNN_ADASYN1.predict(features_test)))\n",
    "print('5:8 RF TEST')\n",
    "print(classification_report(targets_test,RF_ADASYN1.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GNB_ADASYN2 = GaussianNB().fit(features_ADASYN2,targets_ADASYN2)\n",
    "KNN_ADASYN2 = KNeighborsClassifier().fit(features_ADASYN2,targets_ADASYN2)\n",
    "RF_ADASYN2 = RandomForestClassifier(n_estimators=64).fit(features_ADASYN2,targets_ADASYN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:1 GNB TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.22      0.73      0.33      8929\n",
      "          1       0.88      0.43      0.58     41545\n",
      "\n",
      "avg / total       0.76      0.49      0.54     50474\n",
      "\n",
      "1:1 kNN TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.18      0.37      0.25      8929\n",
      "          1       0.83      0.64      0.72     41545\n",
      "\n",
      "avg / total       0.71      0.60      0.64     50474\n",
      "\n",
      "1:1 RF TEST\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.39      0.09      0.15      8929\n",
      "          1       0.83      0.97      0.90     41545\n",
      "\n",
      "avg / total       0.75      0.81      0.76     50474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('1:1 GNB TEST')\n",
    "print(classification_report(targets_test,GNB_ADASYN2.predict(features_test)))\n",
    "print('1:1 kNN TEST')\n",
    "print(classification_report(targets_test,KNN_ADASYN2.predict(features_test)))\n",
    "print('1:1 RF TEST')\n",
    "print(classification_report(targets_test,RF_ADASYN2.predict(features_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "GNBtn, GNBfp, GNBfn, GNBtp = confusion_matrix(targets_test,GNB_ADASYN2.predict(features_test)).ravel()\n",
    "kNNtn, kNNfp, kNNfn, kNNtp = confusion_matrix(targets_test,KNN_ADASYN2.predict(features_test)).ravel()\n",
    "RFtn, RFfp, RFfn, RFtp = confusion_matrix(targets_test,RF_ADASYN2.predict(features_test)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "GNBnegrecall = GNBtn/(GNBtn+GNBfp)\n",
    "GNBposrecall = GNBtp/(GNBtp+GNBfn)\n",
    "\n",
    "kNNnegrecall = kNNtn/(kNNtn+kNNfp)\n",
    "kNNposrecall = kNNtp/(kNNtp+kNNfn)\n",
    "\n",
    "RFnegrecall = RFtn/(RFtn+RFfp)\n",
    "RFposrecall = RFtp/(RFtp+RFfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize a summary of metrics and resampling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In summary, it seems there is a necessary tradeoff between model sensitivity towards negative/positive class outputs that is dependent on the balance of training examples. Different resampling approaches offer more or less very similar results on model performance, which is dependent on the final balance between minority/majority class examples. For improved performance (in particular, precision has not improved much after using any of the resampling techniques), it seems we must delve deeper into transforming the input data and exploring more powerful algorithms (w/ hyperparamter optimization)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
