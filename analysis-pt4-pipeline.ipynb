{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select(query):\n",
    "    \n",
    "    conn = sqlite3.connect('./data/lending-club-loan-data/database2.sqlite')\n",
    "    cursor = conn.cursor()\n",
    "    temp_df = pd.DataFrame(cursor.execute(query).fetchall())\n",
    "    temp_df.columns = list(map(lambda x: x[0], cursor.description))\n",
    "    conn.close()\n",
    "    \n",
    "    return temp_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = select('SELECT * FROM FEATURES_TRAIN')\n",
    "targets_train = select('SELECT * FROM TARGETS_TRAIN').loan_status\n",
    "features_test = select('SELECT * FROM FEATURES_TEST')\n",
    "targets_test = select('SELECT * FROM TARGETS_TEST').loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# undersampling ratio, SMOTE oversampling ratio, ADASYN oversampling ratio. all 1:1\n",
    "\n",
    "ratios = [{0:len(features_train[~targets_train.astype(bool)]),\\\n",
    "               1:len(features_train[~targets_train.astype(bool)])},\\\n",
    "         {0:len(features_train[targets_train.astype(bool)]),\\\n",
    "                1:len(features_train[targets_train.astype(bool)])},\\\n",
    "         {0:len(features_train[targets_train.astype(bool)]),\\\n",
    "                1:len(features_train[targets_train.astype(bool)])}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sklearn's Pipeline class requires that intermediary steps have fit and transform methods. our re-samplers\n",
    "# do not have transforms, so we must define a custom pipeline for this specific application.\n",
    "# the function will be fairly limited and specific to our needs for simplicity/effectiveness\n",
    "\n",
    "# the function runs through the whole pipeline twice: without hyperparam optimization and with hyperparam optimization\n",
    "\n",
    "# 'model' argument should be a tuple w/ the first entry as the model's name as a string; the second entry a class instance \n",
    "# model names should be: LR, GNB, KNN, or RF for the function to properly work..\n",
    "\n",
    "# 8 results for each algorithm. note that resampling is w/o a deterministic seed, so results may vary on the same call\n",
    "\n",
    "def Pipeline(features,targets,model,resample_ratios,metric):\n",
    "    \n",
    "    resamplers = [None,RandomUnderSampler,SMOTE,ADASYN]\n",
    "    resampler_names = ['UnderSamp','SMOTE','ADASYN']\n",
    "    output = {}\n",
    "    output['model'] = model[0]\n",
    "    i = 0\n",
    "    \n",
    "    for resampler in resamplers:\n",
    "        if resampler == None:\n",
    "            final_features = features.copy()\n",
    "            final_targets = targets.copy()\n",
    "            tn,fp,fn,tp = confusion_matrix(targets_test,model[1].fit(final_features,final_targets).predict(features_test)).ravel()\n",
    "            \n",
    "            if metric == 'neg_prec':\n",
    "                output['neg_prec'] = tn/(tn+fn)\n",
    "            elif metric == 'neg_recall':\n",
    "                output['neg_recall'] = tn/(tn+fp)\n",
    "            elif metric == 'pos_prec':\n",
    "                output['pos_prec'] = tp/(tp+fp)\n",
    "            elif metric == 'pos_recall':\n",
    "                output['pos_recall'] = tp/(tp+fn)\n",
    "            \n",
    "        else:\n",
    "            final_features, final_targets = resampler(ratio=resample_ratios[i]).fit_sample(features,targets)\n",
    "            tn,fp,fn,tp = confusion_matrix(targets_test,model[1].fit(final_features,final_targets).predict(features_test)).ravel()\n",
    "            \n",
    "            if metric == 'neg_prec':\n",
    "                output[resampler_names[i]] = tn/(tn+fn)\n",
    "            elif metric == 'neg_recall':\n",
    "                output[resampler_names[i]] = tn/(tn+fp)\n",
    "            elif metric == 'pos_prec':\n",
    "                output[resampler_names[i]] = tp/(tp+fp)\n",
    "            elif metric == 'pos_recall':\n",
    "                output[resampler_names[i]] = tp/(tp+fn)\n",
    "                \n",
    "            i+=1\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimize hyperparameters for F1 score on negative class, since that is the area we're looking to improve\n",
    "\n",
    "def neg_f1(targets_true,targets_predicted):\n",
    "    tn, fp, fn, tp = confusion_matrix(targets_true,targets_predicted).ravel()\n",
    "    precision = tn/(tn+fn)\n",
    "    recall = tn/(tn+fp)\n",
    "    return 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "def neg_precision(targets_true,targets_predicted):\n",
    "    tn, fp, fn, tp = confusion_matrix(targets_true,targets_predicted).ravel()\n",
    "    return tn/(tn+fn)\n",
    "\n",
    "def neg_recall(targets_true,targets_predicted):\n",
    "    tn, fp, fn, tp = confusion_matrix(targets_true,targets_predicted).ravel()\n",
    "    return tn/(tn+fp)\n",
    "\n",
    "def pos_precision(targets_true,targets_predicted):\n",
    "    tn, fp, fn, tp = confusion_matrix(targets_true,targets_predicted).ravel()\n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def pos_recall(targets_true,targets_predicted):\n",
    "    tn, fp, fn, tp = confusion_matrix(targets_true,targets_predicted).ravel()\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "# the metric to optimize for in the regularized runs of the pipeline will determine the appropriate score function\n",
    "\n",
    "score_dict = {'neg_prec':neg_precision,'neg_recall':neg_recall,'pos_prec':pos_precision,'pos_recall':pos_recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will leave out RF. we saw in the analysis of regularization that changing the params from the defaults\n",
    "# greatly reduces model performance across all metrics\n",
    "\n",
    "def Pipeline1(features,targets,model,resample_ratios,metric):\n",
    "    \n",
    "    resamplers = [None,RandomUnderSampler,SMOTE,ADASYN]\n",
    "    resampler_names = ['UnderSamp','SMOTE','ADASYN']\n",
    "    output = {}\n",
    "    output['model'] = 're'+model[0]\n",
    "    i = 0\n",
    "    \n",
    "    if model[0] == 'LR':\n",
    "        params = {'C':[.001,.01,.1,1,10,100]}\n",
    "    elif model[0] == 'GNB': \n",
    "        params = {'priors':[[0.1,0.9],[0.2,0.8],[0.3,0.7],[0.4,0.6],[0.5,0.5],[0.6,0.4]]}\n",
    "    elif model[0] == 'KNN':\n",
    "        params = {'n_neighbors':list(range(3,8))}\n",
    "    elif model[0] == 'RF':\n",
    "        params = {'min_samples_split':[2,8,32],'min_samples_leaf':[1,16,32]}\n",
    "    \n",
    "    for resampler in resamplers:\n",
    "        if resampler == None:\n",
    "            final_features = features.copy()\n",
    "            final_targets = targets.copy()\n",
    "            clf = GridSearchCV(model[1],param_grid=params,scoring=make_scorer(score_dict[metric]),return_train_score=True)\\\n",
    "                                .fit(final_features,final_targets)\n",
    "            tn,fp,fn,tp = confusion_matrix(targets_test,clf.best_estimator_.predict(features_test)).ravel()\n",
    "            \n",
    "            if metric == 'neg_prec':\n",
    "                output['neg_prec'] = tn/(tn+fn)\n",
    "            elif metric == 'neg_recall':\n",
    "                output['neg_recall'] = tn/(tn+fp)\n",
    "            elif metric == 'pos_prec':\n",
    "                output['pos_prec'] = tp/(tp+fp)\n",
    "            elif metric == 'pos_recall':\n",
    "                output['pos_recall'] = tp/(tp+fn)\n",
    "            \n",
    "        else:\n",
    "            final_features, final_targets = resampler(ratio=resample_ratios[i]).fit_sample(features,targets)\n",
    "            clf = GridSearchCV(model[1],param_grid=params,scoring=make_scorer(score_dict[metric]),return_train_score=True)\\\n",
    "                                .fit(final_features,final_targets)\n",
    "            tn,fp,fn,tp = confusion_matrix(targets_test,clf.best_estimator_.predict(features_test)).ravel()\n",
    "            \n",
    "            if metric == 'neg_prec':\n",
    "                output[resampler_names[i]] = tn/(tn+fn)\n",
    "            elif metric == 'neg_recall':\n",
    "                output[resampler_names[i]] = tn/(tn+fp)\n",
    "            elif metric == 'pos_prec':\n",
    "                output[resampler_names[i]] = tp/(tp+fp)\n",
    "            elif metric == 'pos_recall':\n",
    "                output[resampler_names[i]] = tp/(tp+fn)\n",
    "                \n",
    "            i+=1\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defines a pipeline process at a higher level of abstraction, which runs both the unregularized and regularized \n",
    "# version of the algorithm, returning the final desired 2x8 matrix with results across different sampling methods.\n",
    "\n",
    "# the column w/ the metric name as the column name is the un-resampled version\n",
    "\n",
    "def FinalPipeline(model,metric,ratio_dicts):\n",
    "    unregularized = Pipeline(features=features_train,targets=targets_train,\\\n",
    "                             model=model,resample_ratios=ratio_dicts,metric=metric)\n",
    "    \n",
    "    regularized = Pipeline1(features=features_train,targets=targets_train,\\\n",
    "                            model=model,resample_ratios=ratio_dicts,metric=metric)\n",
    "    return pd.DataFrame(unregularized,index=[0]).set_index('model')\\\n",
    "                    .append(pd.DataFrame(regularized,index=[0]).set_index('model')).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as the functions are currently defined, they only provide a shallow evaluation of which algorithms perform\n",
    "# best on certain metrics. the best parameters are not output...\n",
    "\n",
    "\n",
    "# !!\n",
    "# perhaps we want to return two metrics at a time, so we can see how much we sacrifice in a metric's performance\n",
    "# in exchange for an optimal value in another metric. as defined now, FinalPipeline returns results for just one metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADASYN</th>\n",
       "      <th>SMOTE</th>\n",
       "      <th>UnderSamp</th>\n",
       "      <th>neg_prec</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.271674</td>\n",
       "      <td>0.273748</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reLR</th>\n",
       "      <td>0.256313</td>\n",
       "      <td>0.272523</td>\n",
       "      <td>0.270718</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ADASYN     SMOTE  UnderSamp  neg_prec\n",
       "model                                         \n",
       "LR     0.253800  0.271674   0.273748  0.500000\n",
       "reLR   0.256313  0.272523   0.270718  0.466667"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we see below that for logistic regression, hyperparameter optimization does little to improve the model performance\n",
    "# the majority of change in the model's performance comes from resampling\n",
    "\n",
    "FinalPipeline(('LR',LogisticRegression()),'neg_prec',ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADASYN</th>\n",
       "      <th>SMOTE</th>\n",
       "      <th>UnderSamp</th>\n",
       "      <th>neg_recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.726845</td>\n",
       "      <td>0.642177</td>\n",
       "      <td>0.640497</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reLR</th>\n",
       "      <td>0.724829</td>\n",
       "      <td>0.635793</td>\n",
       "      <td>0.643745</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ADASYN     SMOTE  UnderSamp  neg_recall\n",
       "model                                           \n",
       "LR     0.726845  0.642177   0.640497    0.000224\n",
       "reLR   0.724829  0.635793   0.643745    0.000224"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we see that regularization does not guarantee performance boosts on unseen data. particularly for logistic regression,\n",
    "# whose performance barely improves with regularization in this context, we see slightly worse performance after regularization\n",
    "# on 2/3 re-sampling methods\n",
    "\n",
    "FinalPipeline(('LR',LogisticRegression()),'neg_recall',ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADASYN</th>\n",
       "      <th>SMOTE</th>\n",
       "      <th>UnderSamp</th>\n",
       "      <th>neg_prec</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GNB</th>\n",
       "      <td>0.217168</td>\n",
       "      <td>0.254352</td>\n",
       "      <td>0.264051</td>\n",
       "      <td>0.325714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reGNB</th>\n",
       "      <td>0.356580</td>\n",
       "      <td>0.345704</td>\n",
       "      <td>0.340388</td>\n",
       "      <td>0.354498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ADASYN     SMOTE  UnderSamp  neg_prec\n",
       "model                                         \n",
       "GNB    0.217168  0.254352   0.264051  0.325714\n",
       "reGNB  0.356580  0.345704   0.340388  0.354498"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for Gaussian NB, regularization has a more notable effect on the model's performance for both the following two metrics\n",
    "\n",
    "FinalPipeline(('GNB',GaussianNB()),'neg_prec',ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADASYN</th>\n",
       "      <th>SMOTE</th>\n",
       "      <th>UnderSamp</th>\n",
       "      <th>neg_recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GNB</th>\n",
       "      <td>0.730653</td>\n",
       "      <td>0.659312</td>\n",
       "      <td>0.645985</td>\n",
       "      <td>0.307985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reGNB</th>\n",
       "      <td>0.786874</td>\n",
       "      <td>0.753500</td>\n",
       "      <td>0.748236</td>\n",
       "      <td>0.722589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ADASYN     SMOTE  UnderSamp  neg_recall\n",
       "model                                           \n",
       "GNB    0.730653  0.659312   0.645985    0.307985\n",
       "reGNB  0.786874  0.753500   0.748236    0.722589"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FinalPipeline(('GNB',GaussianNB()),'neg_recall',ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# negative precision seems to be the metric most difficult to optimize for.. we can \"artifically\" improve neg recall\n",
    "# by increasing the model's sensitivity to negative outputs (via resampling), but it's quite difficult for \n",
    "# the model to be precise in its prediction of negative classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
