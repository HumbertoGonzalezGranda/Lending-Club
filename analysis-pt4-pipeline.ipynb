{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select(query):\n",
    "    \n",
    "    conn = sqlite3.connect('./data/lending-club-loan-data/database2.sqlite')\n",
    "    cursor = conn.cursor()\n",
    "    temp_df = pd.DataFrame(cursor.execute(query).fetchall())\n",
    "    temp_df.columns = list(map(lambda x: x[0], cursor.description))\n",
    "    conn.close()\n",
    "    \n",
    "    return temp_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = select('SELECT * FROM FEATURES_TRAIN')\n",
    "targets_train = select('SELECT * FROM TARGETS_TRAIN').loan_status\n",
    "features_test = select('SELECT * FROM FEATURES_TEST')\n",
    "targets_test = select('SELECT * FROM TARGETS_TEST').loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# undersampling ratio, SMOTE oversampling ratio, ADASYN oversampling ratio. all 1:1\n",
    "\n",
    "ratios = [{0:len(features_train[~targets_train.astype(bool)]),\\\n",
    "               1:len(features_train[~targets_train.astype(bool)])},\\\n",
    "         {0:len(features_train[targets_train.astype(bool)]),\\\n",
    "                1:len(features_train[targets_train.astype(bool)])},\\\n",
    "         {0:len(features_train[targets_train.astype(bool)]),\\\n",
    "                1:len(features_train[targets_train.astype(bool)])}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sklearn's Pipeline class requires that intermediary steps have fit and transform methods. our re-samplers\n",
    "# do not have transforms, so we must define a custom pipeline for this specific application.\n",
    "# the function will be fairly limited and specific to our needs for simplicity/effectiveness\n",
    "\n",
    "# the function runs through the whole pipeline twice: without hyperparam optimization and with hyperparam optimization\n",
    "\n",
    "# 'model' argument should be a tuple w/ the first entry as the model's name as a string; the second entry a class instance \n",
    "# model names should be: LR, GNB, KNN, or RF for the function to properly work..\n",
    "\n",
    "# 8 results for each algorithm. note that resampling is w/o a deterministic seed, so results may vary on the same call\n",
    "\n",
    "def Pipeline0(features,targets,model,resample_ratios,metric):\n",
    "    \n",
    "    resamplers = [None,RandomUnderSampler,SMOTE,ADASYN]\n",
    "    resampler_names = ['UnderSamp','SMOTE','ADASYN']\n",
    "    output = {}\n",
    "    output['model'] = model[0]\n",
    "    i = 0\n",
    "    \n",
    "    for resampler in resamplers:\n",
    "        if resampler == None:\n",
    "            final_features = features.copy()\n",
    "            final_targets = targets.copy()\n",
    "            tn,fp,fn,tp = confusion_matrix(targets_test,model[1].fit(final_features,final_targets).predict(features_test)).ravel()\n",
    "            \n",
    "            if metric == 'neg':\n",
    "                output['neg_prec'] = tn/(tn+fn)\n",
    "                output['neg_recall'] = tn/(tn+fp)\n",
    "            elif metric == 'pos':\n",
    "                output['pos_prec'] = tp/(tp+fp)\n",
    "                output['pos_recall'] = tp/(tp+fn)\n",
    "            \n",
    "        else:\n",
    "            final_features, final_targets = resampler(ratio=resample_ratios[i]).fit_sample(features,targets)\n",
    "            tn,fp,fn,tp = confusion_matrix(targets_test,model[1].fit(final_features,final_targets).predict(features_test)).ravel()\n",
    "            \n",
    "            if metric == 'neg':\n",
    "                output[resampler_names[i]+'_prec'] = tn/(tn+fn)\n",
    "                output[resampler_names[i]+'_recall'] = tn/(tn+fp)\n",
    "            elif metric == 'pos':\n",
    "                output[resampler_names[i]+'_prec'] = tp/(tp+fp)\n",
    "                output[resampler_names[i]+'_recall'] = tp/(tp+fn)\n",
    "                \n",
    "            i+=1\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimize hyperparameters for F1 score on negative class, since that is the area we're looking to improve\n",
    "\n",
    "def neg_f1(targets_true,targets_predicted):\n",
    "    tn, fp, fn, tp = confusion_matrix(targets_true,targets_predicted).ravel()\n",
    "    precision = tn/(tn+fn)\n",
    "    recall = tn/(tn+fp)\n",
    "    return 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "def pos_f1(targets_true,targets_predicted):\n",
    "    tn, fp, fn, tp = confusion_matrix(targets_true,targets_predicted).ravel()\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    return 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "# the metric to optimize for in the regularized runs of the pipeline will determine the appropriate score function\n",
    "\n",
    "score_dict = {'neg':neg_f1,'pos':pos_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will leave out RF. we saw in the analysis of regularization that changing the params from the defaults\n",
    "# greatly reduces model performance across all metrics\n",
    "\n",
    "# we can actually just incorporate this function into pipeline0, using control flow..\n",
    "\n",
    "def Pipeline1(features,targets,model,resample_ratios,metric):\n",
    "    \n",
    "    resamplers = [None,RandomUnderSampler,SMOTE,ADASYN]\n",
    "    resampler_names = ['UnderSamp','SMOTE','ADASYN']\n",
    "    output = {}\n",
    "    output['model'] = 're'+model[0]\n",
    "    i = 0\n",
    "    \n",
    "    if model[0] == 'LR':\n",
    "        params = {'C':[.001,.01,.1,1,10,100]}\n",
    "    elif model[0] == 'GNB': \n",
    "        params = {'priors':[[0.1,0.9],[0.2,0.8],[0.3,0.7],[0.4,0.6],[0.5,0.5],[0.6,0.4]]}\n",
    "    elif model[0] == 'KNN':\n",
    "        params = {'n_neighbors':list(range(3,8))}\n",
    "    elif model[0] == 'RF':\n",
    "        params = {'min_samples_split':[2,8,32],'min_samples_leaf':[1,16,32]}\n",
    "    \n",
    "    for resampler in resamplers:\n",
    "        if resampler == None:\n",
    "            final_features = features.copy()\n",
    "            final_targets = targets.copy()\n",
    "            clf = GridSearchCV(model[1],param_grid=params,scoring=make_scorer(score_dict[metric]),return_train_score=True)\\\n",
    "                                .fit(final_features,final_targets)\n",
    "            tn,fp,fn,tp = confusion_matrix(targets_test,clf.best_estimator_.predict(features_test)).ravel()\n",
    "            \n",
    "            if metric == 'neg':\n",
    "                output['neg_prec'] = tn/(tn+fn)\n",
    "                output['neg_recall'] = tn/(tn+fp)\n",
    "            elif metric == 'pos':\n",
    "                output['pos_prec'] = tp/(tp+fp)\n",
    "                output['pos_recall'] = tp/(tp+fn)\n",
    "            \n",
    "        else:\n",
    "            final_features, final_targets = resampler(ratio=resample_ratios[i]).fit_sample(features,targets)\n",
    "            clf = GridSearchCV(model[1],param_grid=params,scoring=make_scorer(score_dict[metric]),return_train_score=True)\\\n",
    "                                .fit(final_features,final_targets)\n",
    "            tn,fp,fn,tp = confusion_matrix(targets_test,clf.best_estimator_.predict(features_test)).ravel()\n",
    "            \n",
    "            if metric == 'neg':\n",
    "                output[resampler_names[i]+'_prec'] = tn/(tn+fn)\n",
    "                output[resampler_names[i]+'_recall'] = tn/(tn+fp)\n",
    "            elif metric == 'pos':\n",
    "                output[resampler_names[i]+'_prec'] = tp/(tp+fp)\n",
    "                output[resampler_names[i]+'_recall'] = tp/(tp+fn)\n",
    "                \n",
    "            i+=1\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defines a pipeline process at a higher level of abstraction, which runs both the unregularized and regularized \n",
    "# version of the algorithm, returning the final desired 2x8 matrix with results across different sampling methods.\n",
    "\n",
    "# the column w/ the metric name as the column name is the un-resampled version\n",
    "\n",
    "def PipelineLayer1(model,metric,ratio_dicts):\n",
    "    unregularized = Pipeline0(features=features_train,targets=targets_train,\\\n",
    "                             model=model,resample_ratios=ratio_dicts,metric=metric)\n",
    "    \n",
    "    regularized = Pipeline1(features=features_train,targets=targets_train,\\\n",
    "                            model=model,resample_ratios=ratio_dicts,metric=metric)\n",
    "    return pd.DataFrame(unregularized,index=[0]).set_index('model')\\\n",
    "                    .append(pd.DataFrame(regularized,index=[0]).set_index('model')).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# algs_list should be a list of tuples (tuples formatted as the 'model' argument are entered into the lower level pipelines)\n",
    "# e.g. ('LR',LogisticRegression())\n",
    "\n",
    "# 'metric' is changed to pos/neg. we always want to study precision and recall together. for the actual analysis,\n",
    "# we would run this function twice, to view study the performance of the models on negative classes and positive classes\n",
    "# we avoid returning all the metrics at once, because we're left with a table of 16 columns..too much information to\n",
    "# easily digest from a human standpoint\n",
    "\n",
    "def PipelineLayer2(model_list,metric,ratios):\n",
    "    \n",
    "    tempdf = pd.DataFrame()\n",
    "    \n",
    "    for model in model_list:\n",
    "        tempdf = tempdf.append(PipelineLayer1(model,metric,ratios))\n",
    "        \n",
    "    return tempdf.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as the functions are currently defined, they only provide a shallow evaluation of which algorithms perform\n",
    "# best on certain metrics. the best parameters are not output...\n",
    "\n",
    "\n",
    "# !!\n",
    "# perhaps we want to return two metrics at a time, so we can see how much we sacrifice in a metric's performance\n",
    "# in exchange for an optimal value in another metric. as defined now, FinalPipeline returns results for just one metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR_GNB = PipelineLayer2([('LR',LogisticRegression()),('GNB',GaussianNB())],metric='neg',ratios=ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADASYN_prec</th>\n",
       "      <th>ADASYN_recall</th>\n",
       "      <th>SMOTE_prec</th>\n",
       "      <th>SMOTE_recall</th>\n",
       "      <th>UnderSamp_prec</th>\n",
       "      <th>UnderSamp_recall</th>\n",
       "      <th>neg_prec</th>\n",
       "      <th>neg_recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.256227</td>\n",
       "      <td>0.715422</td>\n",
       "      <td>0.274333</td>\n",
       "      <td>0.633218</td>\n",
       "      <td>0.273617</td>\n",
       "      <td>0.635457</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reLR</th>\n",
       "      <td>0.256057</td>\n",
       "      <td>0.716094</td>\n",
       "      <td>0.274093</td>\n",
       "      <td>0.631314</td>\n",
       "      <td>0.273093</td>\n",
       "      <td>0.638033</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GNB</th>\n",
       "      <td>0.216318</td>\n",
       "      <td>0.734013</td>\n",
       "      <td>0.252288</td>\n",
       "      <td>0.669840</td>\n",
       "      <td>0.265030</td>\n",
       "      <td>0.611715</td>\n",
       "      <td>0.325714</td>\n",
       "      <td>0.307985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reGNB</th>\n",
       "      <td>0.206968</td>\n",
       "      <td>0.788442</td>\n",
       "      <td>0.235967</td>\n",
       "      <td>0.752828</td>\n",
       "      <td>0.240546</td>\n",
       "      <td>0.743756</td>\n",
       "      <td>0.263459</td>\n",
       "      <td>0.608915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ADASYN_prec  ADASYN_recall  SMOTE_prec  SMOTE_recall  UnderSamp_prec  \\\n",
       "model                                                                         \n",
       "LR        0.256227       0.715422    0.274333      0.633218        0.273617   \n",
       "reLR      0.256057       0.716094    0.274093      0.631314        0.273093   \n",
       "GNB       0.216318       0.734013    0.252288      0.669840        0.265030   \n",
       "reGNB     0.206968       0.788442    0.235967      0.752828        0.240546   \n",
       "\n",
       "       UnderSamp_recall  neg_prec  neg_recall  \n",
       "model                                          \n",
       "LR             0.635457  0.500000    0.000224  \n",
       "reLR           0.638033  0.500000    0.000224  \n",
       "GNB            0.611715  0.325714    0.307985  \n",
       "reGNB          0.743756  0.263459    0.608915  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we see below that for logistic regression, hyperparameter optimization does little to improve the model performance\n",
    "# the majority of change in the model's performance comes from resampling\n",
    "\n",
    "# PipelineLayer1(('LR',LogisticRegression()),'neg',ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for Gaussian NB, regularization has a more notable effect on the model's performance for both the following two metrics\n",
    "\n",
    "# PipelineLayer1(('GNB',GaussianNB()),'neg',ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# negative precision seems to be the metric most difficult to optimize for.. we can \"artifically\" improve neg recall\n",
    "# by increasing the model's sensitivity to negative outputs (via resampling), but it's quite difficult for \n",
    "# the model to be precise in its prediction of negative classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
